# Modern Docker: A Full-Stack Tutorial with BuildKit, Multi-Stage Builds, Compose, GitHub Actions, GHCR, and Azure

> **What you will build:** A containerised task-management application with a lightweight Python/FastAPI backend (SQLite) and an nginx-served HTML/JS frontend â€” developed in GitHub Codespaces, tested and shipped automatically via GitHub Actions, images stored in GHCR, and finally deployed to Azure Container Apps.

---

## Table of Contents

1. [Why a "Modern" Stack?](#1-why-a-modern-stack)
2. [The Five Concepts Explained](#2-the-five-concepts-explained)
   - 2.1 [BuildKit](#21-buildkit)
   - 2.2 [Multi-Stage Dockerfiles](#22-multi-stage-dockerfiles)
   - 2.3 [Docker Compose](#23-docker-compose)
   - 2.4 [CI/CD with GitHub Actions](#24-cicd-with-github-actions)
   - 2.5 [GHCR â€” GitHub Container Registry](#25-ghcr--github-container-registry)
3. [Architecture Overview](#3-architecture-overview)
4. [Phase 1 â€” Create and Clone the GitHub Repository](#4-phase-1--create-and-clone-the-github-repository)
5. [Phase 2 â€” Write the Application Code](#5-phase-2--write-the-application-code)
   - 5.1 [Backend: FastAPI + SQLite](#51-backend-fastapi--sqlite)
   - 5.2 [Frontend: Static HTML/JS served by nginx](#52-frontend-static-htmljs-served-by-nginx)
6. [Phase 3 â€” Write Multi-Stage Dockerfiles](#6-phase-3--write-multi-stage-dockerfiles)
   - 6.1 [Backend Dockerfile](#61-backend-dockerfile)
   - 6.2 [Frontend Dockerfile](#62-frontend-dockerfile)
   - 6.3 [.dockerignore files](#63-dockerignore-files)
7. [Phase 4 â€” Docker Compose for Local Development](#7-phase-4--docker-compose-for-local-development)
8. [Phase 5 â€” GitHub Codespaces Setup](#8-phase-5--github-codespaces-setup)
9. [Phase 6 â€” GitHub Actions CI/CD Pipeline](#9-phase-6--github-actions-cicd-pipeline)
   - 9.1 [CI Workflow: Test and Build](#91-ci-workflow-test-and-build)
   - 9.2 [CD Workflow: Publish to GHCR](#92-cd-workflow-publish-to-ghcr)
   - 9.3 [Deploy Workflow: Push to Azure](#93-deploy-workflow-push-to-azure)
10. [Phase 7 â€” Deploy to Azure Container Apps](#10-phase-7--deploy-to-azure-container-apps)
11. [Best Practices Reference](#11-best-practices-reference)
12. [Technology Choice Guide](#12-technology-choice-guide)
13. [Further Reading](#13-further-reading)

---

## 1. Why a "Modern" Stack?

The original Docker tutorial covers the basics: pull an image, run a container, push it to Azure Container Registry (ACR). That workflow is correct, but it reflects practices from around 2018â€“2020. Today's professional approach layers in five additional technologies that address real problems:

| Problem | Old approach | Modern answer |
|---|---|---|
| Slow, non-deterministic builds | `docker build` (BuildKit off by default in older Docker) | BuildKit via `docker buildx build` |
| Large, insecure production images | Single-stage Dockerfile (SDK + app together) | Multi-stage Dockerfile (build â‰  runtime) |
| "How do I run all my containers together?" | Manual `docker run` chains | Docker Compose |
| "Who built this image, and was it tested?" | Ad-hoc scripts | GitHub Actions (or Buildkite, CircleCI, etc.) |
| "Where do I store images without paying for ACR?" | Azure Container Registry | GHCR (free tier, tied to your GitHub repo) |

This tutorial works through all five of these in order, in the context of a real (if simple) application.

---

## 2. The Five Concepts Explained

### 2.1 BuildKit

**What is it?**

BuildKit is Docker's next-generation build subsystem, rewritten from scratch. It replaced the original "classic" builder and has been the default since Docker Desktop 23.0 (2023) and Docker Engine 23.0. However, calling it *explicitly* via `docker buildx build` gives you access to features not yet surfaced through the legacy `docker build` alias.

**Why does it matter?**

| Feature | Classic builder | BuildKit |
|---|---|---|
| Parallel stage execution | No | Yes |
| Layer cache import/export | No | Yes (`--cache-from`, `--cache-to`) |
| Secret mounts (e.g. SSH keys, API tokens) at build time | No | Yes (`--secret`) |
| SSH forwarding into build | No | Yes (`--ssh`) |
| Cross-platform images | No | Yes (`--platform linux/amd64,linux/arm64`) |
| Inline cache in image | No | Yes (`--cache-to type=inline`) |

**Key `docker buildx` flags used in this tutorial:**

```bash
docker buildx build \
  --platform linux/amd64 \        # target platform
  --cache-from type=gha \         # read GitHub Actions cache
  --cache-to   type=gha,mode=max \ # write GitHub Actions cache
  --tag  ghcr.io/ORG/IMAGE:TAG \  # destination tag
  --push \                         # push directly to registry
  --file backend/Dockerfile \      # explicit path to Dockerfile
  backend/                         # build context
```

> **âš ï¸ Subtle difference from `docker build`:** When you call `docker buildx build`, the build runs in a BuildKit *builder instance* (a separate daemon or container), not in the local Docker daemon. This means the resulting image is **not automatically available** in `docker image ls` unless you add `--load` (single-platform) or `--push`. You cannot both `--load` and `--push` simultaneously for multi-platform builds.

**Further reading:** [BuildKit documentation](https://docs.docker.com/build/buildkit/) Â· [docker buildx build reference](https://docs.docker.com/reference/cli/docker/buildx/build/)

---

### 2.2 Multi-Stage Dockerfiles

**What is it?**

A Dockerfile can contain multiple `FROM` statements, each starting a new *stage*. Earlier stages can be used as named sources for `COPY --from=<stage>` instructions in later stages. The final image contains only what is in the *last* stage (or a named stage you specify with `--target`).

**Why does it matter?**

Without multi-stage builds, a Python application's production image would contain pip, the compiler toolchain, build caches, test dependencies, and the `.git` directory â€” all of which an attacker could exploit, and all of which bloat the image. Multi-stage builds let you:

- **Build** in a fat SDK image (compilers, linters, test runners present)
- **Copy only the output artefacts** into a minimal runtime image

**Conceptual diagram:**

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Stage 1: "builder"          â”‚  â”€â”€â”€â–º â”‚  Stage 2: "runtime"         â”‚
 â”‚  python:3.12                 â”‚       â”‚  python:3.12-slim           â”‚
 â”‚  - pip install all deps      â”‚       â”‚  - COPY --from=builder      â”‚
 â”‚  - run tests                 â”‚       â”‚    /install /usr/local       â”‚
 â”‚  - compile wheels            â”‚       â”‚  - CMD ["uvicorn", ...]      â”‚
 â”‚  (stays hidden, discarded)   â”‚       â”‚  (this becomes the image)   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Image size comparison (real-world example):**

| Approach | Image size |
|---|---|
| Single-stage (python:3.12 full) | ~1.1 GB |
| Multi-stage (python:3.12-slim runtime) | ~130 MB |
| Multi-stage (python:3.12-alpine runtime) | ~65 MB |

> **âš ï¸ Alpine pitfall:** Alpine uses `musl libc` instead of `glibc`. Some Python packages (especially those with C extensions, like `pandas`, `numpy`, or `cryptography`) must be recompiled from source on Alpine, dramatically increasing build time and sometimes failing entirely. For most web services, `python:3.12-slim-bookworm` (Debian Bookworm slim) is the better default.

**Further reading:** [Multi-stage builds](https://docs.docker.com/build/building/multi-stage/) Â· [Best practices for writing Dockerfiles](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)

---

### 2.3 Docker Compose

**What is it?**

Docker Compose is a tool for defining and running multi-container applications. You describe your entire application stack â€” services, networks, volumes, environment variables, health checks, and port bindings â€” in a single YAML file (`compose.yaml` or `docker-compose.yml`). One command (`docker compose up`) builds images if needed and starts everything in the correct order.

> **Note on naming:** The original `docker-compose` (v1) was a separate Python binary. The modern `docker compose` (v2) is a Go plugin built into the Docker CLI. The old binary is deprecated. This tutorial uses `docker compose` (v2). [Migration guide](https://docs.docker.com/compose/migrate/).

**Key Compose concepts:**

| Concept | Purpose |
|---|---|
| `services` | Each container is a service with its own build/image config |
| `networks` | Named virtual networks; services on the same network resolve each other by service name as a hostname |
| `volumes` | Named persistent storage; used here to persist SQLite data across container restarts |
| `depends_on` | Controls startup order; add a `condition: service_healthy` for proper dependency management |
| `healthcheck` | A command Docker runs periodically to determine if a service is ready |
| `env_file` / `environment` | Pass configuration without baking secrets into images |

**Typical development vs production split:**

```
compose.yaml              â† base (shared config)
compose.override.yaml     â† automatically merged for local dev (hot-reload, debug ports)
compose.prod.yaml         â† explicit merge for production: docker compose -f compose.yaml -f compose.prod.yaml up
```

**Further reading:** [Docker Compose overview](https://docs.docker.com/compose/) Â· [Compose file reference](https://docs.docker.com/reference/compose-file/)

---

### 2.4 CI/CD with GitHub Actions

**What is it?**

GitHub Actions is a workflow automation platform built into GitHub. Workflows are YAML files stored in `.github/workflows/`. They run on GitHub-hosted (or self-hosted) runners in response to events â€” pushes, pull requests, scheduled triggers, or manual dispatch.

**Why is it central to Docker?**

Before pushing an image to a registry or deploying to a cloud, you want automated assurance that:

1. The code compiles / tests pass
2. The Docker image builds successfully
3. The image is tagged correctly (Git SHA, semver, `latest`)
4. Secrets (registry credentials, cloud tokens) are never stored in code

GitHub Actions solves all four. The key Docker-related actions are:

| Action | Purpose |
|---|---|
| `docker/setup-buildx-action` | Installs and configures BuildKit builder |
| `docker/login-action` | Authenticates to GHCR (or Docker Hub, ACR, etc.) |
| `docker/metadata-action` | Generates tags and labels from Git context |
| `docker/build-push-action` | Builds with BuildKit and pushes in one step, with cache |

**GitHub Actions vs alternatives:**

| Tool | Best for |
|---|---|
| **GitHub Actions** | Teams already on GitHub; free for public repos; tight integration with GHCR |
| **Buildkite** | Large organisations needing self-hosted runners with fine-grained control |
| **CircleCI** | Teams wanting advanced caching and parallelism outside GitHub ecosystem |
| **GitLab CI** | Teams on GitLab |
| **Azure Pipelines** | Teams in the Microsoft/Azure ecosystem |

For this tutorial (GitHub repo + GHCR + Azure), GitHub Actions is the natural choice.

**Further reading:** [GitHub Actions docs](https://docs.github.com/en/actions) Â· [Docker GitHub Actions](https://docs.docker.com/build/ci/github-actions/)

---

### 2.5 GHCR â€” GitHub Container Registry

**What is it?**

GHCR (`ghcr.io`) is GitHub's container registry. Images are stored at `ghcr.io/<github-username-or-org>/<image-name>:<tag>`. It uses the same authentication as GitHub (Personal Access Tokens or the automatic `GITHUB_TOKEN` in Actions).

**GHCR vs alternatives:**

| Registry | Free private images? | Free bandwidth? | Notes |
|---|---|---|---|
| **GHCR** | Yes (linked to GitHub storage quota) | Yes for public | Best default for GitHub-based projects |
| **Docker Hub** | 1 private only | Limited for free tier | Rate limits on pulls for anonymous users |
| **Azure Container Registry (ACR)** | No (paid) | Charged | Best when deploying exclusively to Azure |
| **Amazon ECR Public** | Yes | Yes | Best for AWS-native deployments |
| **Google Artifact Registry** | No | Charged | Best for GCP-native deployments |

**Why choose GHCR for this tutorial?**

- The repository is on GitHub, so `GITHUB_TOKEN` already has permission to push â€” no extra secrets needed for CI
- Free tier is generous
- Images can later be pulled by Azure without extra credentials (using a PAT or a service principal)
- You retain the option to make images public (for open-source projects) without changing your workflow

**Further reading:** [Working with GHCR](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) Â· [Authenticating to GHCR](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry)

---

## 3. Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Developer machine / GitHub Codespace                   â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  frontend    â”‚ â”€â”€â”€â”€â”€â”€â–ºâ”‚  backend                  â”‚  â”‚
â”‚  â”‚  nginx:80    â”‚  HTTP  â”‚  FastAPI:8000              â”‚  â”‚
â”‚  â”‚  (static     â”‚        â”‚  + SQLite file             â”‚  â”‚
â”‚  â”‚   HTML/JS)   â”‚        â”‚  (mounted volume)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                          â”‚                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€ Docker network â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ git push
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Actions                                         â”‚
â”‚  1. Run tests (pytest)                                  â”‚
â”‚  2. Build images (BuildKit)                             â”‚
â”‚  3. Push to GHCR (ghcr.io/â€¦)                           â”‚
â”‚  4. Deploy to Azure Container Apps                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Azure Container Apps (production)                      â”‚
â”‚  frontend app  â”€â”€â–º  backend app (+ Azure File volume)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Application: Task Manager**

The application is a minimal task list with two screens:
- A browser UI (served by the frontend container) lists tasks and lets you add/delete them
- The frontend calls a REST API on the backend container
- The backend stores tasks in a SQLite file, persisted on a Docker volume

---

## 4. Phase 1 â€” Create and Clone the GitHub Repository

### 4.1 Create the repository on GitHub

1. Go to [github.com/new](https://github.com/new)
2. Name it `taskapp` (or your preferred name)
3. Check **Add a README file** and set **.gitignore** to **Python**
4. Choose **Private** (you can make it public later)
5. Click **Create repository**

### 4.2 Open in GitHub Codespaces

Codespaces gives you a fully configured cloud development environment. You will configure it further in Phase 5, but you can start immediately:

1. On your repository page, click **Code â†’ Codespaces â†’ Create codespace on main**
2. A VS Code environment opens in your browser; this is where all subsequent work is done

> **Pitfall:** Codespaces uses your GitHub storage quota (usually 15 GB free). Delete unused codespaces at [github.com/codespaces](https://github.com/codespaces) to avoid charges.

### 4.3 Set up the project directory structure

In the Codespace terminal:

```bash
mkdir -p backend/app frontend/.devcontainer .github/workflows
touch backend/Dockerfile backend/requirements.txt backend/.dockerignore
touch backend/app/__init__.py backend/app/main.py backend/app/models.py backend/app/database.py
touch frontend/Dockerfile frontend/.dockerignore
mkdir -p frontend/html
touch frontend/html/index.html
touch compose.yaml compose.override.yaml
touch .devcontainer/devcontainer.json
```

Your final directory tree will look like this:

```
taskapp/
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json          â† Codespaces configuration
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                 â† CI: test + build on every push/PR
â”‚       â””â”€â”€ deploy.yml             â† CD: push to GHCR + deploy to Azure on main
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile                 â† multi-stage Dockerfile for backend
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .dockerignore
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py                â† FastAPI application
â”‚       â”œâ”€â”€ models.py              â† SQLAlchemy models
â”‚       â””â”€â”€ database.py            â† SQLite connection
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile                 â† multi-stage Dockerfile for frontend
â”‚   â”œâ”€â”€ .dockerignore
â”‚   â””â”€â”€ html/
â”‚       â””â”€â”€ index.html             â† single-page UI
â”œâ”€â”€ compose.yaml                   â† base Compose file
â””â”€â”€ compose.override.yaml          â† local dev overrides (hot-reload, etc.)
```

---

## 5. Phase 2 â€” Write the Application Code

### 5.1 Backend: FastAPI + SQLite

**`backend/requirements.txt`**

```
fastapi==0.111.0
uvicorn[standard]==0.29.0
sqlalchemy==2.0.30
```

Keep this list minimal and **pin every version**. Unpinned dependencies are one of the most common causes of "it worked yesterday" build failures.

> **âš ï¸ Pitfall â€” why pin versions?** If you write `fastapi` without a version, pip resolves to the latest at build time. Two builds a week apart can produce different images. Use `pip-tools` (`pip-compile`) or `uv` to generate a fully locked `requirements.txt` from a `requirements.in` file. See [pip-tools](https://pip-tools.readthedocs.io/en/latest/).

**`backend/app/database.py`**

```python
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, DeclarativeBase

# The SQLite file will be stored at /data/tasks.db inside the container.
# This path is mounted as a Docker volume so data persists across restarts.
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:////data/tasks.db")

engine = create_engine(
    DATABASE_URL,
    # SQLite requires this flag when accessed from multiple threads
    # (FastAPI uses a thread pool for sync code).
    connect_args={"check_same_thread": False},
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


class Base(DeclarativeBase):
    pass


def get_db():
    """FastAPI dependency: yields a database session, then closes it."""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

**`backend/app/models.py`**

```python
from sqlalchemy import Column, Integer, String, Boolean
from .database import Base


class Task(Base):
    __tablename__ = "tasks"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String(200), nullable=False)
    done = Column(Boolean, default=False)
```

**`backend/app/main.py`**

```python
from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sqlalchemy.orm import Session

from .database import Base, engine, get_db
from .models import Task

# Create database tables on startup (simple approach; use Alembic for production migrations)
Base.metadata.create_all(bind=engine)

app = FastAPI(title="Task API", version="1.0.0")

# CORS: allow the frontend container (and Codespaces preview ports) to call the API.
# In production, replace "*" with your actual frontend URL.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


# â”€â”€ Pydantic schemas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TaskCreate(BaseModel):
    title: str


class TaskResponse(BaseModel):
    id: int
    title: str
    done: bool

    model_config = {"from_attributes": True}


# â”€â”€ Routes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/healthz")
def health_check():
    """Used by Docker health checks and Azure liveness probes."""
    return {"status": "ok"}


@app.get("/tasks", response_model=list[TaskResponse])
def list_tasks(db: Session = Depends(get_db)):
    return db.query(Task).all()


@app.post("/tasks", response_model=TaskResponse, status_code=201)
def create_task(payload: TaskCreate, db: Session = Depends(get_db)):
    task = Task(title=payload.title)
    db.add(task)
    db.commit()
    db.refresh(task)
    return task


@app.patch("/tasks/{task_id}", response_model=TaskResponse)
def toggle_task(task_id: int, db: Session = Depends(get_db)):
    task = db.get(Task, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    task.done = not task.done
    db.commit()
    db.refresh(task)
    return task


@app.delete("/tasks/{task_id}", status_code=204)
def delete_task(task_id: int, db: Session = Depends(get_db)):
    task = db.get(Task, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    db.delete(task)
    db.commit()
```

### 5.2 Frontend: Static HTML/JS served by nginx

The frontend is a single HTML file. It uses `fetch()` to call the backend API. The `BACKEND_URL` is set to an empty string by default (so that `/tasks` becomes a relative URL handled by nginx's reverse proxy configuration), making it environment-agnostic.

**`frontend/html/index.html`**

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Task Manager</title>
  <style>
    body { font-family: system-ui, sans-serif; max-width: 600px; margin: 2rem auto; padding: 0 1rem; }
    h1   { color: #2563eb; }
    input[type=text] { width: 70%; padding: .5rem; font-size: 1rem; border: 1px solid #ccc; border-radius: 4px; }
    button { padding: .5rem 1rem; cursor: pointer; border-radius: 4px; border: none; background: #2563eb; color: white; font-size: 1rem; }
    button.danger { background: #dc2626; }
    li   { display: flex; align-items: center; gap: .5rem; padding: .4rem 0; border-bottom: 1px solid #eee; }
    .done { text-decoration: line-through; color: #9ca3af; }
  </style>
</head>
<body>
  <h1>ğŸ“‹ Task Manager</h1>
  <div style="display:flex;gap:.5rem;margin-bottom:1.5rem">
    <input type="text" id="newTask" placeholder="Add a new taskâ€¦" />
    <button onclick="addTask()">Add</button>
  </div>
  <ul id="taskList"></ul>

  <script>
    // nginx proxies /api/ â†’ http://backend:8000/
    // so we never hard-code the backend hostname in HTML.
    const API = '/api';

    async function loadTasks() {
      const res = await fetch(`${API}/tasks`);
      const tasks = await res.json();
      const list = document.getElementById('taskList');
      list.innerHTML = '';
      tasks.forEach(t => {
        const li = document.createElement('li');
        li.innerHTML = `
          <input type="checkbox" ${t.done ? 'checked' : ''} onchange="toggleTask(${t.id})">
          <span class="${t.done ? 'done' : ''}">${t.title}</span>
          <button class="danger" onclick="deleteTask(${t.id})" style="margin-left:auto">âœ•</button>
        `;
        list.appendChild(li);
      });
    }

    async function addTask() {
      const input = document.getElementById('newTask');
      const title = input.value.trim();
      if (!title) return;
      await fetch(`${API}/tasks`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ title }),
      });
      input.value = '';
      await loadTasks();
    }

    async function toggleTask(id) {
      await fetch(`${API}/tasks/${id}`, { method: 'PATCH' });
      await loadTasks();
    }

    async function deleteTask(id) {
      await fetch(`${API}/tasks/${id}`, { method: 'DELETE' });
      await loadTasks();
    }

    // Enter key support
    document.getElementById('newTask').addEventListener('keydown', e => {
      if (e.key === 'Enter') addTask();
    });

    loadTasks();
  </script>
</body>
</html>
```

**Why route through nginx instead of calling the backend directly?**

If the frontend called `http://backend:8000` directly, it would work inside Docker's network but fail in a browser (the browser cannot resolve `backend` as a hostname). By having nginx act as a reverse proxy, the browser always talks to the same origin (the frontend), and nginx forwards `/api/*` requests to the backend container internally.

---

## 6. Phase 3 â€” Write Multi-Stage Dockerfiles

### 6.1 Backend Dockerfile

**`backend/Dockerfile`**

```dockerfile
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Stage 1 â€” builder
# Uses the full Python image so pip can compile any C-extension packages.
# This stage is discarded; nothing from here ends up in the final image
# except the installed packages we explicitly copy.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FROM python:3.12-slim-bookworm AS builder

# Set working directory for all subsequent commands in this stage
WORKDIR /build

# Install build dependencies (gcc etc.) only in the builder stage.
# --no-install-recommends keeps this layer smaller.
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copy only the dependency file first.
# Docker layer caching means this layer (and the pip install below)
# will only re-run when requirements.txt actually changes â€” not on every
# code change. This is one of the most impactful Dockerfile optimisations.
COPY requirements.txt .

# Install packages into /install so we can copy them cleanly to the runtime stage.
# --prefix puts everything (scripts + site-packages) under /install.
RUN pip install --no-cache-dir --prefix=/install -r requirements.txt


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Stage 2 â€” runtime
# Minimal image: no build tools, no pip, no compiler.
# An attacker who breaks in cannot install new tools easily.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FROM python:3.12-slim-bookworm AS runtime

# OCI standard labels â€” appear in 'docker inspect' and on GHCR package page
LABEL org.opencontainers.image.source="https://github.com/YOUR_ORG/taskapp"
LABEL org.opencontainers.image.description="Task Manager backend"
LABEL org.opencontainers.image.licenses="MIT"

WORKDIR /app

# Copy the installed packages from the builder stage.
# /install/lib/python3.12/site-packages â†’ /usr/local/lib/python3.12/site-packages
COPY --from=builder /install /usr/local

# Copy the application source code.
# This layer changes on every code edit, but it's tiny â€” so rebuilds are fast.
COPY app/ app/

# Create the /data directory where SQLite will store its file.
# We do this in the Dockerfile so the directory exists even before
# the volume is mounted; otherwise SQLite cannot create the database file.
RUN mkdir -p /data

# Run as a non-root user â€” best practice for production containers.
# If an attacker exploits the app, they get a low-privilege user, not root.
RUN useradd --system --create-home appuser && chown -R appuser /data /app
USER appuser

# Document the port the application listens on.
# EXPOSE does not publish the port; it is metadata for humans and tooling.
EXPOSE 8000

# Health check: Docker (and Compose, and Azure) will poll this endpoint.
# --interval: how often to check
# --timeout: how long to wait for a response
# --retries: failures before marking unhealthy
# --start-period: grace period after container start before checks count
HEALTHCHECK --interval=15s --timeout=5s --retries=3 --start-period=10s \
  CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8000/healthz')"

# Start the application with uvicorn.
# Use "0.0.0.0" to listen on all interfaces inside the container.
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Annotated layer order rationale:**

```
FROM          â† base image (cached by Docker registry)
RUN apt-get   â† build tools (changes rarely â†’ cache-stable layer)
COPY req.txt  â† only the dep file (changes occasionally)
RUN pip       â† deps install (invalidated only when req.txt changes)
COPY app/     â† source code (changes frequently, but this layer is small)
```

This order maximises cache hits. If you put `COPY . .` before `RUN pip install`, every code change forces a full pip reinstall.

### 6.2 Frontend Dockerfile

The frontend uses two stages: a Node.js stage for any future build step (minification, bundling), and an nginx stage to serve the result. Even though the current frontend is a single plain HTML file, structuring it this way means you can add a bundler (Vite, Parcel) later without restructuring the Dockerfile.

**`frontend/Dockerfile`**

```dockerfile
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Stage 1 â€” builder
# Placeholder for a future build step (npm run build, minification, etc.)
# For now it just copies the HTML file into /dist.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FROM node:20-alpine AS builder

WORKDIR /build

# If you later add package.json, npm install would go here.
# For now, just copy the static files into the expected output directory.
COPY html/ /dist/


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Stage 2 â€” runtime
# nginx:alpine is ~23 MB â€” a very small production image.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FROM nginx:1.27-alpine AS runtime

LABEL org.opencontainers.image.source="https://github.com/YOUR_ORG/taskapp"
LABEL org.opencontainers.image.description="Task Manager frontend"

# Copy the static files from the builder stage into nginx's document root.
COPY --from=builder /dist /usr/share/nginx/html

# Replace the default nginx config with our custom one that adds
# a reverse proxy for /api/ requests to the backend container.
COPY nginx.conf /etc/nginx/conf.d/default.conf

EXPOSE 80

HEALTHCHECK --interval=15s --timeout=5s --retries=3 \
  CMD wget -qO- http://localhost/healthz || exit 1
```

**`frontend/nginx.conf`** (create this file alongside the Dockerfile)

```nginx
server {
    listen 80;
    server_name _;

    # Serve static files from the document root
    root /usr/share/nginx/html;
    index index.html;

    # Health check endpoint for Docker / Azure
    location /healthz {
        return 200 'ok';
        add_header Content-Type text/plain;
    }

    # Reverse-proxy /api/ requests to the backend container.
    # Inside Docker's network, "backend" resolves to the backend container
    # because Compose puts both on the same named network and uses the
    # service name as the DNS hostname.
    location /api/ {
        # Strip the /api prefix before forwarding.
        rewrite ^/api/(.*) /$1 break;
        proxy_pass         http://backend:8000;
        proxy_set_header   Host              $host;
        proxy_set_header   X-Real-IP         $remote_addr;
        proxy_set_header   X-Forwarded-For   $proxy_add_x_forwarded_for;
        proxy_set_header   X-Forwarded-Proto $scheme;
    }

    # Catch-all: serve index.html for client-side routing
    location / {
        try_files $uri $uri/ /index.html;
    }
}
```

### 6.3 `.dockerignore` files

`.dockerignore` works exactly like `.gitignore` but for Docker build contexts. Without it, `COPY . .` sends your entire directory (including `.git`, `__pycache__`, `node_modules`, virtual environments) to the BuildKit daemon, which is slow and can accidentally leak secrets.

**`backend/.dockerignore`**

```
# Python artefacts
__pycache__/
*.py[cod]
*.egg-info/
.venv/
venv/
env/

# Test and CI artefacts
.pytest_cache/
.coverage
htmlcov/
.mypy_cache/

# Local data (never bake the SQLite file into the image)
*.db
*.sqlite

# Git
.git/
.gitignore
```

**`frontend/.dockerignore`**

```
node_modules/
.git/
*.md
```

---

## 7. Phase 4 â€” Docker Compose for Local Development

### 7.1 Base Compose file

**`compose.yaml`**

```yaml
# compose.yaml â€” base configuration used in all environments.
# Local dev merges this with compose.override.yaml (automatic).
# Production uses: docker compose -f compose.yaml -f compose.prod.yaml up

name: taskapp   # prefix for container/network/volume names

services:

  backend:
    build:
      context: ./backend          # build context is the backend directory
      dockerfile: Dockerfile      # explicit, even though it's the default
      # BuildKit cache: inline cache is stored in the image itself.
      # This means any machine that pulls the image can use it as a cache source.
      cache_from:
        - type=registry,ref=ghcr.io/YOUR_ORG/taskapp-backend:cache
    image: ghcr.io/YOUR_ORG/taskapp-backend:dev
    environment:
      # Override the default SQLite path via environment variable
      DATABASE_URL: sqlite:////data/tasks.db
    volumes:
      # Named volume: persists the SQLite file between container restarts.
      # Without this, all tasks are lost whenever the container stops.
      - sqlite_data:/data
    networks:
      - app_net
    healthcheck:
      test: ["CMD", "python", "-c",
             "import urllib.request; urllib.request.urlopen('http://localhost:8000/healthz')"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    image: ghcr.io/YOUR_ORG/taskapp-frontend:dev
    ports:
      # Expose the frontend to the host on port 8080.
      # The backend is NOT exposed to the host â€” it's only reachable
      # within Docker's internal network (through nginx's proxy).
      - "8080:80"
    networks:
      - app_net
    depends_on:
      backend:
        # Wait until the backend's health check passes before starting nginx.
        # Without this condition, nginx might start and proxy /api/ before
        # the backend is ready, causing 502 errors on first load.
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost/healthz"]
      interval: 15s
      timeout: 5s
      retries: 3
    restart: unless-stopped

networks:
  app_net:
    # A named bridge network. Containers on this network resolve each other
    # by their service name (e.g. "backend", "frontend").
    driver: bridge

volumes:
  sqlite_data:
    # A named volume managed by Docker. Data survives container removal.
    # To fully reset: docker compose down -v
```

### 7.2 Development overrides

**`compose.override.yaml`**

This file is **automatically merged** with `compose.yaml` when you run `docker compose up` locally. You do not need to specify it explicitly. It adds development-only features.

```yaml
# compose.override.yaml â€” local development overrides.
# Automatically merged by Docker Compose. Do NOT commit secrets here;
# use a .env file (gitignored) for local secrets.

services:

  backend:
    build:
      # Enable BuildKit's inline cache for local builds too.
      # 'target' lets you build only the builder stage for debugging:
      #   docker compose build --build-arg TARGET=builder backend
      args:
        BUILDKIT_INLINE_CACHE: "1"
    # Mount the source code as a volume so that code changes are reflected
    # without rebuilding the image. Uvicorn's --reload flag picks up changes.
    volumes:
      - ./backend/app:/app/app   # source override
      - sqlite_data:/data         # keep data volume too
    # Override CMD to enable hot-reload in development
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    # Expose the backend port directly in dev so you can call the API
    # from the host (e.g. with curl or Postman) without going through nginx.
    ports:
      - "8000:8000"

  frontend:
    # In dev, also expose port 80 directly if needed for debugging nginx config
    ports:
      - "8080:80"
```

### 7.3 Running locally

```bash
# Build images (BuildKit is used automatically in modern Docker)
docker compose build

# Start everything in detached mode
docker compose up -d

# Watch logs from all services
docker compose logs -f

# Watch logs from only one service
docker compose logs -f backend

# Check health status
docker compose ps
```

**Expected output of `docker compose ps`:**

```
NAME                  IMAGE                                 COMMAND                  SERVICE    CREATED         STATUS                    PORTS
taskapp-backend-1     ghcr.io/YOUR_ORG/taskapp-backend:dev  "uvicorn app.main:apâ€¦"  backend    2 minutes ago   Up 2 minutes (healthy)    0.0.0.0:8000->8000/tcp
taskapp-frontend-1    ghcr.io/YOUR_ORG/taskapp-frontend:dev "nginx -g 'daemon ofâ€¦"  frontend   2 minutes ago   Up 2 minutes (healthy)    0.0.0.0:8080->80/tcp
```

Open `http://localhost:8080` in your browser. You should see the Task Manager UI.

**Test the API directly:**

```bash
# List tasks (empty initially)
curl http://localhost:8000/tasks
# Expected: []

# Create a task
curl -X POST http://localhost:8000/tasks \
     -H "Content-Type: application/json" \
     -d '{"title": "Learn BuildKit"}'
# Expected: {"id":1,"title":"Learn BuildKit","done":false}

# List tasks again
curl http://localhost:8000/tasks
# Expected: [{"id":1,"title":"Learn BuildKit","done":false}]

# Check health
curl http://localhost:8000/healthz
# Expected: {"status":"ok"}
```

**Tear down:**

```bash
# Stop containers but keep volumes (data preserved)
docker compose down

# Stop containers AND delete volumes (full reset)
docker compose down -v
```

> **âš ï¸ Pitfall â€” `depends_on` does not wait for readiness without `condition`:** If you write `depends_on: - backend` (without the `condition: service_healthy` block), Compose only waits until the container *starts*, not until it's *ready to accept connections*. The frontend will often start before the backend's FastAPI server is fully up. Always use `condition: service_healthy` when one service must be ready before another.

---

## 8. Phase 5 â€” GitHub Codespaces Setup

A `devcontainer.json` file tells Codespaces (and VS Code's Dev Containers extension) how to configure the development environment. Without it, Codespaces gives you a generic Ubuntu machine; with it, you get Docker Compose already running, extensions pre-installed, and forwarded ports ready.

**`.devcontainer/devcontainer.json`**

```jsonc
{
  "name": "Task App Dev Environment",

  // Use the Docker-outside-of-Docker feature so the devcontainer can run
  // docker and docker compose commands against the Codespace's Docker daemon.
  // Alternative: "docker-in-docker" (slower, fully isolated).
  "features": {
    "ghcr.io/devcontainers/features/docker-outside-of-docker:1": {},
    "ghcr.io/devcontainers/features/python:1": {
      "version": "3.12"
    }
  },

  // Run docker compose up when the Codespace starts.
  "postStartCommand": "docker compose up -d",

  // Stop containers when the Codespace stops (saves resources).
  "postAttachCommand": "",

  // Forward ports to the Codespace's preview URLs.
  "forwardPorts": [8080, 8000],
  "portsAttributes": {
    "8080": {
      "label": "Frontend (Task Manager UI)",
      "onAutoForward": "openBrowserOnce"
    },
    "8000": {
      "label": "Backend API",
      "onAutoForward": "silent"
    }
  },

  // VS Code extensions installed automatically in the Codespace
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance",
        "ms-azuretools.vscode-docker",
        "timonwong.shellcheck",
        "redhat.vscode-yaml",
        "esbenp.prettier-vscode"
      ],
      "settings": {
        "python.defaultInterpreterPath": "/usr/local/bin/python",
        "editor.formatOnSave": true
      }
    }
  }
}
```

When you (or a team member) opens the repo in Codespaces, the environment will:

1. Spin up a Ubuntu VM
2. Install Docker, Python 3.12, and the listed VS Code extensions
3. Run `docker compose up -d` automatically
4. Forward ports 8080 and 8000 to preview URLs
5. Open a browser tab pointing at the Task Manager UI

> **Codespaces vs local Docker Desktop:** Codespaces is ideal for onboarding new contributors (no "works on my machine" problems), for machines where you cannot install Docker (e.g. corporate lockdown), and for testing on Linux when you develop on macOS or Windows. The tradeoff is latency (you're editing files over HTTPS) and potential cost.

---

## 9. Phase 6 â€” GitHub Actions CI/CD Pipeline

### 9.1 CI Workflow: Test and Build

This workflow runs on every push and pull request. It:
- Installs Python dependencies
- Runs tests with pytest
- Verifies both Docker images build without errors (but does not push)

**`.github/workflows/ci.yml`**

```yaml
name: CI â€” Test and Build

on:
  push:
    branches: ["**"]          # all branches
  pull_request:
    branches: [main]

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest

    steps:
      - name: Check out code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          # Cache pip's download cache to speed up repeat runs.
          # The cache key changes when requirements.txt changes.
          cache: "pip"
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        run: pip install -r backend/requirements.txt pytest httpx

      - name: Run tests
        working-directory: backend
        # PYTHONPATH lets pytest find the app module
        env:
          PYTHONPATH: .
          # Use an in-memory SQLite database for tests (no file I/O needed)
          DATABASE_URL: "sqlite:///:memory:"
        run: pytest tests/ -v

  build:
    name: Build Docker Images
    runs-on: ubuntu-latest
    # Only try to build if tests pass
    needs: test

    steps:
      - name: Check out code
        uses: actions/checkout@v4

      # Install and configure BuildKit builder
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Build the backend image (no push, just verify it builds)
      - name: Build backend image
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: false        # CI: build only, do not push
          tags: taskapp-backend:ci
          # Cache layers in GitHub Actions cache.
          # 'gha' type stores/retrieves from the Actions cache service.
          # 'mode=max' caches all intermediate layers, not just the final one.
          cache-from: type=gha,scope=backend
          cache-to:   type=gha,scope=backend,mode=max

      # Build the frontend image (no push, just verify it builds)
      - name: Build frontend image
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: false
          tags: taskapp-frontend:ci
          cache-from: type=gha,scope=frontend
          cache-to:   type=gha,scope=frontend,mode=max
```

**Adding tests** â€” create a minimal test file to make CI useful:

**`backend/tests/test_main.py`**

```python
import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.main import app
from app.database import Base, get_db

# Use an in-memory SQLite database for tests â€” fast and isolated
TEST_DB_URL = "sqlite:///:memory:"
engine = create_engine(TEST_DB_URL, connect_args={"check_same_thread": False})
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base.metadata.create_all(bind=engine)


def override_get_db():
    db = TestingSessionLocal()
    try:
        yield db
    finally:
        db.close()


# Override the real DB dependency with the test one
app.dependency_overrides[get_db] = override_get_db
client = TestClient(app)


def test_health():
    r = client.get("/healthz")
    assert r.status_code == 200
    assert r.json() == {"status": "ok"}


def test_create_and_list_tasks():
    r = client.post("/tasks", json={"title": "Buy groceries"})
    assert r.status_code == 201
    data = r.json()
    assert data["title"] == "Buy groceries"
    assert data["done"] is False

    r = client.get("/tasks")
    assert r.status_code == 200
    assert len(r.json()) == 1


def test_toggle_task():
    r = client.post("/tasks", json={"title": "Toggle me"})
    task_id = r.json()["id"]

    r = client.patch(f"/tasks/{task_id}")
    assert r.json()["done"] is True

    r = client.patch(f"/tasks/{task_id}")
    assert r.json()["done"] is False


def test_delete_task():
    r = client.post("/tasks", json={"title": "Delete me"})
    task_id = r.json()["id"]

    r = client.delete(f"/tasks/{task_id}")
    assert r.status_code == 204

    r = client.get("/tasks")
    ids = [t["id"] for t in r.json()]
    assert task_id not in ids
```

Also add the tests directory init file:

```bash
touch backend/tests/__init__.py
```

### 9.2 CD Workflow: Publish to GHCR

This workflow runs only when code is merged to `main`. It builds both images with BuildKit, tags them with the Git SHA and `latest`, and pushes them to GHCR.

**`.github/workflows/deploy.yml`** (first section â€” build and push)

```yaml
name: CD â€” Build, Push, and Deploy

on:
  push:
    branches: [main]
  # Allow manual triggering from the GitHub UI
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  # github.repository is "your-username/taskapp"
  IMAGE_BACKEND:  ghcr.io/${{ github.repository_owner }}/taskapp-backend
  IMAGE_FRONTEND: ghcr.io/${{ github.repository_owner }}/taskapp-frontend

jobs:

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Job 1: Build and push images to GHCR
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  build-and-push:
    name: Build and Push to GHCR
    runs-on: ubuntu-latest
    # Output the digest of each image so the deploy job can reference them.
    # Using digests (sha256:abcâ€¦) instead of tags prevents race conditions
    # where "latest" might point to a different image by the time deployment
    # starts (especially important in high-frequency CI pipelines).
    outputs:
      backend-digest:  ${{ steps.push-backend.outputs.digest }}
      frontend-digest: ${{ steps.push-frontend.outputs.digest }}

    permissions:
      contents: read
      packages: write   # required to push to GHCR

    steps:
      - name: Check out code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Authenticate to GHCR using the automatic GITHUB_TOKEN.
      # No manual secret needed â€” Actions grants this permission automatically
      # when the 'packages: write' permission is set above.
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # Generate tags and OCI labels from Git metadata.
      # This action produces tags like:
      #   ghcr.io/org/taskapp-backend:main
      #   ghcr.io/org/taskapp-backend:sha-a1b2c3d
      #   ghcr.io/org/taskapp-backend:latest  (only on default branch)
      - name: Generate backend metadata
        id: meta-backend
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.IMAGE_BACKEND }}
          tags: |
            type=ref,event=branch
            type=sha,prefix=sha-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Generate frontend metadata
        id: meta-frontend
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.IMAGE_FRONTEND }}
          tags: |
            type=ref,event=branch
            type=sha,prefix=sha-
            type=raw,value=latest,enable={{is_default_branch}}

      # Build the backend and push to GHCR.
      # BuildKit uses GitHub's cache service, dramatically speeding up
      # repeat builds (only changed layers are rebuilt).
      - name: Build and push backend
        id: push-backend
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags:    ${{ steps.meta-backend.outputs.tags }}
          labels:  ${{ steps.meta-backend.outputs.labels }}
          cache-from: type=gha,scope=backend
          cache-to:   type=gha,scope=backend,mode=max
          # Build for the platform that Azure Container Apps runs on.
          # If you need ARM64 support too, add: linux/arm64
          platforms: linux/amd64

      - name: Build and push frontend
        id: push-frontend
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags:    ${{ steps.meta-frontend.outputs.tags }}
          labels:  ${{ steps.meta-frontend.outputs.labels }}
          cache-from: type=gha,scope=frontend
          cache-to:   type=gha,scope=frontend,mode=max
          platforms: linux/amd64

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Job 2: Deploy to Azure Container Apps
  # Runs after build-and-push succeeds.
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  deploy:
    name: Deploy to Azure
    runs-on: ubuntu-latest
    needs: build-and-push
    environment: production   # requires manual approval if configured

    steps:
      - name: Azure login
        uses: azure/login@v2
        with:
          # Store the service principal JSON as a GitHub secret named AZURE_CREDENTIALS
          # See Phase 7 for how to create this.
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Deploy backend to Azure Container Apps
        uses: azure/container-apps-deploy-action@v2
        with:
          resourceGroup:        taskapp-rg
          containerAppName:     taskapp-backend
          imageToDeploy:        ${{ env.IMAGE_BACKEND }}@${{ needs.build-and-push.outputs.backend-digest }}
          # Pass the GHCR credentials so Azure can pull the private image
          registryUrl:          ghcr.io
          registryUsername:     ${{ github.actor }}
          registryPassword:     ${{ secrets.GHCR_PAT }}

      - name: Deploy frontend to Azure Container Apps
        uses: azure/container-apps-deploy-action@v2
        with:
          resourceGroup:        taskapp-rg
          containerAppName:     taskapp-frontend
          imageToDeploy:        ${{ env.IMAGE_FRONTEND }}@${{ needs.build-and-push.outputs.frontend-digest }}
          registryUrl:          ghcr.io
          registryUsername:     ${{ github.actor }}
          registryPassword:     ${{ secrets.GHCR_PAT }}
```

**Setting up required GitHub Secrets:**

| Secret name | Value | How to create |
|---|---|---|
| `GITHUB_TOKEN` | Auto-injected by Actions | No setup needed |
| `GHCR_PAT` | Personal Access Token with `read:packages` scope | GitHub â†’ Settings â†’ Developer settings â†’ Personal access tokens |
| `AZURE_CREDENTIALS` | Service principal JSON | See Phase 7 below |

To add a secret: GitHub repo â†’ **Settings** â†’ **Secrets and variables** â†’ **Actions** â†’ **New repository secret**

> **âš ï¸ Pitfall â€” `GITHUB_TOKEN` vs PAT for GHCR pulls:** `GITHUB_TOKEN` has `packages: write` permission in the workflow that builds images, but Azure Container Apps (running *outside* GitHub) cannot use `GITHUB_TOKEN`. You need a separate PAT (Personal Access Token) or a GitHub App token for Azure to pull private images from GHCR. Store it as `GHCR_PAT`.

**Example CI/CD run output (Actions tab):**

```
âœ“ test           â†’ Run Tests           passed in 23s
âœ“ build (CI)     â†’ Build Docker Images passed in 1m 12s  (cache hit: 94%)
âœ“ build-and-push â†’ Build and Push      passed in 2m 03s
  â””â”€ backend:  ghcr.io/myorg/taskapp-backend:sha-a1b2c3d
  â””â”€ frontend: ghcr.io/myorg/taskapp-frontend:sha-a1b2c3d
âœ“ deploy         â†’ Deploy to Azure     passed in 45s
```

### 9.3 Branch Protection and Required Status Checks

After the workflows are in place, protect the `main` branch:

1. GitHub repo â†’ **Settings** â†’ **Branches** â†’ **Add branch protection rule**
2. Set **Branch name pattern** to `main`
3. Check **Require status checks to pass before merging**
4. Add `test` and `build` as required checks
5. Check **Require pull request reviews before merging** (recommended for teams)

This ensures that no code goes to production without passing tests and building successfully.

---

## 10. Phase 7 â€” Deploy to Azure Container Apps

### 10.1 Why Azure Container Apps?

Azure offers several container hosting options. The right choice depends on your needs:

| Service | Best for | Managed? | Cost model |
|---|---|---|---|
| **Azure Container Apps (ACA)** | Microservices, event-driven, scale-to-zero | Fully managed | Per vCPU/memory second |
| **Azure Container Instances (ACI)** | Simple single containers, batch jobs | Fully managed | Per second |
| **Azure Kubernetes Service (AKS)** | Large-scale, full control, complex workloads | Control plane managed | Node pool VMs |
| **Azure App Service** | Web apps with familiar PaaS model | Fully managed | App Service Plan |

For this tutorial, **Azure Container Apps** is the right choice because:
- It supports multiple containers with internal networking (like Compose)
- It scales to zero (no cost when idle)
- Dapr integration is available if you need service meshes later
- It's simpler to operate than AKS

### 10.2 Initial Azure Setup (one-time)

Install the Azure CLI if not already present:

```bash
# On Ubuntu / Codespace
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Verify
az --version
```

Log in and create resources:

```bash
# Log in (opens browser)
az login

# Set your subscription (replace with your subscription ID)
az account set --subscription "YOUR_SUBSCRIPTION_ID"

# Create a resource group in a region near you
az group create \
  --name taskapp-rg \
  --location eastus

# Register the Container Apps provider (only needed once per subscription)
az provider register --namespace Microsoft.App
az provider register --namespace Microsoft.OperationalInsights

# Create a Container Apps environment
# This is the shared infrastructure (virtual network, Log Analytics workspace)
# that your container apps will run in.
az containerapp env create \
  --name taskapp-env \
  --resource-group taskapp-rg \
  --location eastus
```

**Expected output:**

```json
{
  "id": "/subscriptions/.../resourceGroups/taskapp-rg/providers/Microsoft.App/managedEnvironments/taskapp-env",
  "location": "East US",
  "name": "taskapp-env",
  "properties": {
    "provisioningState": "Succeeded"
  }
}
```

### 10.3 Create a Service Principal for GitHub Actions

GitHub Actions needs Azure credentials to deploy. Create a service principal with the minimum necessary permissions:

```bash
# Create a service principal scoped to your resource group only.
# This follows the principle of least privilege â€” if the secret leaks,
# an attacker can only affect your taskapp-rg, not your whole subscription.
az ad sp create-for-rbac \
  --name "taskapp-github-actions" \
  --role contributor \
  --scopes /subscriptions/$(az account show --query id -o tsv)/resourceGroups/taskapp-rg \
  --json-auth
```

Copy the entire JSON output. Add it as the `AZURE_CREDENTIALS` GitHub secret. It looks like:

```json
{
  "clientId": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  "clientSecret": "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
  "subscriptionId": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  "tenantId": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
}
```

> **âš ï¸ Security note:** Rotate this secret periodically and never print it in logs. GitHub Actions automatically masks known secret values in logs, but be cautious about `echo`ing JSON objects that might contain the secret.

### 10.4 Deploy the Backend Container App

```bash
# Create the Azure File Share for SQLite persistence.
# Azure Container Apps does not support Docker named volumes directly;
# instead you use Azure Files (an SMB share backed by Azure Storage).
az storage account create \
  --name taskappstore$RANDOM \
  --resource-group taskapp-rg \
  --sku Standard_LRS

# Note the storage account name
STORAGE_ACCOUNT=$(az storage account list -g taskapp-rg --query '[0].name' -o tsv)

# Create a file share
az storage share create \
  --name sqlite-data \
  --account-name $STORAGE_ACCOUNT

# Get the storage account key
STORAGE_KEY=$(az storage account keys list \
  --account-name $STORAGE_ACCOUNT \
  -g taskapp-rg \
  --query '[0].value' -o tsv)

# Link the file share to the Container Apps environment
az containerapp env storage set \
  --name taskapp-env \
  --resource-group taskapp-rg \
  --storage-name sqlite-storage \
  --azure-file-account-name $STORAGE_ACCOUNT \
  --azure-file-account-key $STORAGE_KEY \
  --azure-file-share-name sqlite-data \
  --access-mode ReadWrite

# Deploy the backend container app
az containerapp create \
  --name taskapp-backend \
  --resource-group taskapp-rg \
  --environment taskapp-env \
  --image ghcr.io/YOUR_ORG/taskapp-backend:latest \
  --registry-server ghcr.io \
  --registry-username YOUR_GITHUB_USERNAME \
  --registry-password YOUR_GHCR_PAT \
  --target-port 8000 \
  --ingress internal \           # internal: not directly exposed to internet
  --min-replicas 0 \             # scale to zero when idle
  --max-replicas 3 \
  --cpu 0.25 \
  --memory 0.5Gi \
  --env-vars DATABASE_URL=sqlite:////data/tasks.db \
  --volume-name sqlite-vol \
  --volume-storage-name sqlite-storage \
  --volume-mount-path /data
```

### 10.5 Deploy the Frontend Container App

```bash
# Get the backend's internal FQDN (used for nginx proxy in production)
BACKEND_FQDN=$(az containerapp show \
  --name taskapp-backend \
  --resource-group taskapp-rg \
  --query "properties.configuration.ingress.fqdn" -o tsv)

echo "Backend internal FQDN: $BACKEND_FQDN"

az containerapp create \
  --name taskapp-frontend \
  --resource-group taskapp-rg \
  --environment taskapp-env \
  --image ghcr.io/YOUR_ORG/taskapp-frontend:latest \
  --registry-server ghcr.io \
  --registry-username YOUR_GITHUB_USERNAME \
  --registry-password YOUR_GHCR_PAT \
  --target-port 80 \
  --ingress external \           # external: publicly accessible
  --min-replicas 0 \
  --max-replicas 3 \
  --cpu 0.25 \
  --memory 0.5Gi
```

> **âš ï¸ nginx backend URL in production:** In Azure Container Apps, the backend's internal URL is different from `http://backend:8000` (the Docker Compose hostname). You will need to update `nginx.conf` to use the Azure Container Apps internal FQDN, or pass it as an environment variable and use `envsubst` to substitute it at container start. A practical approach is to use environment variables and an entrypoint script.

**Production nginx entrypoint approach** â€” add to `frontend/Dockerfile`:

```dockerfile
# In the runtime stage, add an entrypoint that substitutes
# the BACKEND_URL environment variable into nginx.conf
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh
ENTRYPOINT ["/entrypoint.sh"]
CMD ["nginx", "-g", "daemon off;"]
```

**`frontend/entrypoint.sh`**:

```bash
#!/bin/sh
# Substitute $BACKEND_URL in nginx.conf template, then start nginx
envsubst '$BACKEND_URL' < /etc/nginx/conf.d/default.conf.template \
  > /etc/nginx/conf.d/default.conf
exec "$@"
```

And update `nginx.conf` to use `${BACKEND_URL}` as the proxy target.

### 10.6 Accessing the deployed application

```bash
# Get the public URL of the frontend
az containerapp show \
  --name taskapp-frontend \
  --resource-group taskapp-rg \
  --query "properties.configuration.ingress.fqdn" -o tsv
```

Output: `taskapp-frontend.wittyrock-abc123.eastus.azurecontainerapps.io`

Open this URL in your browser to see the production Task Manager.

### 10.7 Viewing logs in Azure

```bash
# Stream live logs from the backend
az containerapp logs show \
  --name taskapp-backend \
  --resource-group taskapp-rg \
  --follow

# View recent logs
az containerapp logs show \
  --name taskapp-backend \
  --resource-group taskapp-rg \
  --tail 50
```

---

## 11. Best Practices Reference

### Image security

| Practice | How |
|---|---|
| Run as non-root | `RUN useradd appuser && USER appuser` in Dockerfile |
| Minimal base images | Use `-slim` or `-alpine` variants |
| No secrets in images | Use `--secret` mount at build time; use env vars at runtime |
| Scan images for vulnerabilities | `docker scout cves IMAGE` or integrate Trivy in CI |
| Pin base image digests | `FROM python:3.12-slim-bookworm@sha256:abcâ€¦` for maximum reproducibility |

### Build performance

| Practice | How |
|---|---|
| Copy dependency files before source | `COPY requirements.txt .` then `RUN pip install` then `COPY app/ .` |
| Use `.dockerignore` | Exclude `__pycache__`, `.git`, `node_modules`, `*.db` |
| Use BuildKit cache mounts | `RUN --mount=type=cache,target=/root/.cache/pip pip install â€¦` |
| Use registry cache in CI | `--cache-from type=gha --cache-to type=gha,mode=max` |

### Compose best practices

| Practice | How |
|---|---|
| Use health checks with `depends_on` | `condition: service_healthy` instead of bare `depends_on` |
| Use named volumes for persistence | Avoids accidental data loss; survives container replacement |
| Don't expose backend ports in production | Only expose what users need (the frontend port) |
| Use `.env` for local secrets | Add `.env` to `.gitignore`; never commit credentials |

### CI/CD best practices

| Practice | How |
|---|---|
| Require tests to pass before building | Use `needs: test` in the build job |
| Pin action versions | `uses: actions/checkout@v4` not `@main` |
| Use image digests for deployments | Reference `IMAGE@sha256:â€¦` not `IMAGE:latest` |
| Protect the main branch | Require PR reviews + passing CI before merge |
| Use environments with approval gates | Require a human to approve production deployments |

---

## 12. Technology Choice Guide

### "Which database should I use?"

SQLite is excellent for: solo projects, prototypes, read-heavy applications, applications where the DB is on the same machine as the app, and embedded devices. Its limitations: single writer at a time (WAL mode helps), not suitable for high write concurrency, not natively supported across multiple container replicas.

For multi-replica deployments (where you need more than one backend container), move to **PostgreSQL** (self-hosted or Azure Database for PostgreSQL). The SQLAlchemy code in this tutorial requires only a `DATABASE_URL` change to switch.

### "Should I use Docker Compose in production?"

Docker Compose is well-suited for small deployments on a single VM. For production you should use:
- **Azure Container Apps** (this tutorial) â€” for simple microservices up to ~20 services
- **Azure Kubernetes Service (AKS)** â€” when you need full Kubernetes (custom networking, advanced scheduling, >20 services, large teams)
- **Docker Swarm** â€” still viable but largely superseded by Kubernetes

### "When should I use GHCR vs ACR?"

Use **GHCR** when: your code is on GitHub, you want a free tier, you're working on an open-source project, or you're a small team.

Use **ACR** when: you need tight Azure RBAC integration (Managed Identity auth from AKS/ACA without a PAT), you need geo-replication of images, or your organisation has an Azure-first policy.

### "When should I use GitHub Actions vs Azure Pipelines?"

Use **GitHub Actions** when: your code is on GitHub (easiest integration, `GITHUB_TOKEN` works natively with GHCR).

Use **Azure Pipelines** when: your code is in Azure DevOps, or you need tighter integration with Azure-specific features like secure variable groups and Azure Artifact Feeds.

---

## 13. Further Reading

### BuildKit and Docker Build
- [BuildKit documentation](https://docs.docker.com/build/buildkit/) â€” official reference
- [docker buildx build CLI reference](https://docs.docker.com/reference/cli/docker/buildx/build/)
- [BuildKit cache mounts and secrets](https://docs.docker.com/build/building/secrets/)
- [Multi-platform builds](https://docs.docker.com/build/building/multi-platform/)

### Multi-Stage Dockerfiles
- [Multi-stage builds](https://docs.docker.com/build/building/multi-stage/) â€” official guide
- [Dockerfile best practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/) â€” layer ordering, caching
- [distroless images](https://github.com/GoogleContainerTools/distroless) â€” even smaller than Alpine, no shell at all

### Docker Compose
- [Docker Compose overview](https://docs.docker.com/compose/)
- [Compose file specification](https://docs.docker.com/reference/compose-file/)
- [Compose Watch (live sync without volume mounts)](https://docs.docker.com/compose/how-tos/file-watch/)
- [Migrating from Compose v1 to v2](https://docs.docker.com/compose/migrate/)

### GitHub Actions
- [GitHub Actions documentation](https://docs.github.com/en/actions)
- [Docker's official GitHub Actions guide](https://docs.docker.com/build/ci/github-actions/)
- [docker/build-push-action](https://github.com/docker/build-push-action) â€” the core action
- [docker/metadata-action](https://github.com/docker/metadata-action) â€” tag/label generation
- [Reusable workflows](https://docs.github.com/en/actions/using-workflows/reusing-workflows)

### GHCR
- [Working with the Container Registry](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry)
- [Authenticating to GHCR in Actions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-with-a-personal-access-token-classic)

### GitHub Codespaces
- [Codespaces overview](https://docs.github.com/en/codespaces/overview)
- [Dev container configuration](https://containers.dev/implementors/json_reference/) â€” devcontainer.json spec
- [Available dev container features](https://containers.dev/features)

### Azure Container Apps
- [Azure Container Apps documentation](https://learn.microsoft.com/en-us/azure/container-apps/overview)
- [Connect to storage in Azure Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/storage-mounts)
- [Deploy from GHCR to Azure Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/containers)
- [Scale rules in Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/scale-app)

### Security
- [Docker Scout (vulnerability scanning)](https://docs.docker.com/scout/)
- [Trivy (open-source scanner)](https://aquasecurity.github.io/trivy/)
- [OCI image spec labels](https://specs.opencontainers.org/image-spec/annotations/)
- [SLSA (Supply-chain Levels for Software Artifacts)](https://slsa.dev/) â€” provenance attestation

### Python / FastAPI tooling
- [FastAPI documentation](https://fastapi.tiangolo.com/)
- [SQLAlchemy 2.0](https://docs.sqlalchemy.org/en/20/)
- [pip-tools (dependency locking)](https://pip-tools.readthedocs.io/)
- [uv (modern Python package manager, much faster than pip)](https://docs.astral.sh/uv/)
- [Alembic (database migrations for SQLAlchemy)](https://alembic.sqlalchemy.org/en/latest/)

---

*Tutorial version 1.0 Â· February 2026 Â· Covers Docker Engine 26+, Compose v2, GitHub Actions runner ubuntu-latest, Azure Container Apps GA*
